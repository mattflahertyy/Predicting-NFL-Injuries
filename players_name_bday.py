# -*- coding: utf-8 -*-
"""PLAYERS_NAME_BDAY.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pX1zZUBoNWPQ8xkMRNC-wsnAyJGOa1-5
"""

import pandas as pd

from google.colab import files
uploaded=files.upload()

player_data_A= pd.read_csv('players_data_A.csv')
player_data_B= pd.read_csv('players_data_B.csv')
player_data_CH= pd.read_csv('players_data_CDEFGH.csv')
player_data_IQ= pd.read_csv('players_data_IJKLMNOPQ.csv')
player_data_RS= pd.read_csv('players_data_RSTUVWXYZ.csv')

# Concatenate all datasets
all_player_data = pd.concat([player_data_A,player_data_B,player_data_CH,player_data_IQ,player_data_RS], ignore_index=True)

print(all_player_data)

"""27491 rows  total

cleaning this dataframe

step 1) Clean out all the null rows


step 2)  simplify birthday to just year


Step 3)remove all players born before 1968. we are only concerned with players that were active between 2009-2023, and the oldest active player in 2009 was born in 1968 so we can remove all before this


step 4)) resolve duplicate entities by replacing them with a single entry with their name, and the birthday year being teh average of all teh duplicate entries year value.
"""

# Count the null values in each column
null_counts = all_player_data.isnull().sum()

# Display the null counts for each column
print(null_counts)

# Remove rows with null values
all_player_data = all_player_data.dropna()

# Display the updated DataFrame
print(all_player_data)

# Convert the 'Birthday' column to datetime format, handling errors
# WILL make all bdays w invalid format NaT
all_player_data['Birthday'] = pd.to_datetime(all_player_data['Birthday'], errors='coerce')

# Drop rows with 'NaT' values in the 'Birthday' column
all_player_data = all_player_data.dropna(subset=['Birthday'])

# Convert the 'Birthday' column to year and make it an integer
all_player_data['Birthday'] = all_player_data['Birthday'].dt.year.astype(int)

# Print the modified DataFrame
print(all_player_data)

# Filter out rows with invalid dates and where the year of birthday is before 1968
all_player_data = all_player_data[(all_player_data['Birthday'].notnull()) & (all_player_data['Birthday'] >= 1968)]

# Display the updated DataFrame
print(all_player_data)

"""after removing all rows before 1960 , we have 15591 rows

step 4: entity resolution.

we are going to resolve duplicate names into a single entity
"""

# Find duplicate names
duplicate_names = all_player_data[all_player_data.duplicated(subset=['Name'], keep=False)]

# Print duplicate names
print("Duplicate Names:")
print(duplicate_names)

# Create a new DataFrame to store duplicate names and their counts
duplicate_counts = pd.DataFrame(columns=['Name', 'Count'])

# Loop through each unique duplicate name
for name in duplicate_names['Name'].unique():
    # Count occurrences of the current name in the original DataFrame
    count = len(all_player_data[all_player_data['Name'] == name])

   # Append the name and count to the new DataFrame
    duplicate_counts = pd.concat([duplicate_counts, pd.DataFrame({'Name': [name], 'Count': [count]})], ignore_index=True)

# Print the new DataFrame with duplicate counts
print(duplicate_counts)

# Group by 'Name' and calculate the average birthday
grouped_data = all_player_data.groupby('Name')['Birthday'].mean()

# Round the average to the nearest whole number
grouped_data = grouped_data.round().astype(int)

# Create a dictionary to map the original names to the new average birthdays
name_to_avg_birthday = grouped_data.to_dict()

# Replace the original 'Birthday' values with the new average birthdays
all_player_data['Birthday'] = all_player_data['Name'].map(name_to_avg_birthday)

# Drop duplicate rows based on 'Name'
all_player_data = all_player_data.drop_duplicates(subset=['Name'])

# Reset the index
all_player_data = all_player_data.reset_index(drop=True)

# Print the updated DataFrame
print(all_player_data)

# Check if there are any duplicate names
has_duplicates = all_player_data.duplicated(subset=['Name']).any()

# Print the result
if has_duplicates:
    print("Yes, there are duplicate names.")
else:
    print("No, there are no duplicate names.")

all_player_data['Name'] = all_player_data['Name'].replace('Mike Mitchell', 'Michael Mitchell')

"""MERGING WITH THE INJURY DATA"""

from google.colab import files
uploaded=files.upload()

all_injury_data= pd.read_csv('all_injury_data_new (1).csv')

# Replace "Maurice Hurst" with "Maurice Hurst Jr"
all_injury_data['full_name'] = all_injury_data['full_name'].replace('Maurice Hurst', 'Maurice Hurst Jr.')
all_injury_data['full_name'] = all_injury_data['full_name'].replace('Irv Smith', 'Irv Smith Jr.')
all_injury_data['full_name'] = all_injury_data['full_name'].replace('Mark Fields', 'Mark Fields II')
all_injury_data['full_name'] = all_injury_data['full_name'].replace('Lonnie Johnson', 'Lonnie Johnson Jr.')
all_injury_data['full_name'] = all_injury_data['full_name'].replace('Tyrone Wheatley', 'Tyron Wheatley Jr.')
all_injury_data['full_name'] = all_injury_data['full_name'].replace('Jon Runyan', 'Jon Runyan Jr.')
all_injury_data['full_name'] = all_injury_data['full_name'].replace('Michael Pittman', 'Michael Pittman Jr.')
all_injury_data['full_name'] = all_injury_data['full_name'].replace('Irv Smith', 'Irv Smith Jr.')

print(all_injury_data)

"""MERGE THE TWO TABLES"""

#-------------------------#-------------------------#-------------------------#-------------------------#-------------------------
all_player_data.rename(columns={'Name': 'full_name'}, inplace=True)

print(all_player_data)

#-------------------------#-------------------------#-------------------------#-------------------------#-------------------------
# Perform INNER join on 'full_name' and 'Name' columns
merged_data = pd.merge(all_injury_data, all_player_data, on='full_name', how='inner')

# Display the merged DataFrame
print(merged_data)

# Perform left join on 'full_name' and 'Name' columns
merged_data = all_injury_data.merge(all_player_data, how='left', left_on='full_name', right_on='Name')

# Drop the duplicate 'Name' column if needed
merged_data = merged_data.drop('Name', axis=1)

# Display the merged DataFrame
print(merged_data)

# Check for null values in the merged DataFrame
null_counts_merged = merged_data.isnull().sum()

# Display the null counts for each column in the merged DataFrame
print(null_counts_merged)

# Filter rows with null birthdate
null_birthdate_rows = merged_data[merged_data['Birthday'].isnull()]

# Display the rows with null birthdate
print(null_birthdate_rows)

"""drop the rows with a null birthday"""

# Drop rows with null birthday
merged_data = merged_data.dropna(subset=['Birthday'])

# Display the updated DataFrame
print(merged_data)

"""ADD THE AGE COLUMN"""

# Assuming 'season' and 'Birthday' are in datetime format
merged_data['age'] = merged_data['season'].astype(int) - merged_data['Birthday'].astype(int)

# Display the updated DataFrame
print(merged_data)

#------------------#------------------#------------------#------------------#------------------#------------------
sorted_data = merged_data.sort_values(by='age', ascending=False)

# Print the top 5 rows with the highest age
top_5_highest_age = sorted_data.head(30)
print(top_5_highest_age[['full_name', 'Birthday', 'age', 'season']])

#------------------#------------------#------------------#------------------#------------------#------------------#------------------
players_above_49 = merged_data[merged_data['age'] > 49]

# Print players above 49
print(players_above_49[['full_name', 'Birthday', 'age', 'season']])

# Loop through all columns and check for null values
for column in merged_data.columns:
    null_count = merged_data[column].isnull().sum()
    print(f"Column '{column}' has {null_count} null values.")

# Alternatively, you can check for null values in the entire DataFrame
if merged_data.isnull().values.any():
    print("There are null values in the DataFrame.")
else:
    print("No null values found in the DataFrame.")

"""save values to csv"""

merged_data.to_csv('updated_injury_player_data.csv', index=False)